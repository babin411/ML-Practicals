{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intimate-insert",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    Name: Babin Joshi <br/>\n",
    "    Roll No: 19\n",
    "</h3>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-polls",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <div>\n",
    "        <h2>Chapter 3: Clustering - Finding Related Posts</h2>\n",
    "    </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-distinction",
   "metadata": {},
   "source": [
    "In the previous chapter, you learned how to find the classes or categories of\n",
    "individual datapoints. With a handful of training data items that were paired with\n",
    "their respective classes, you learned a model, which we can now use to classify\n",
    "future data items. We called this supervised learning because the learning was\n",
    "guided by a teacher; in our case, the teacher had the form of correct classifications.<br/><br/>\n",
    "Let's now imagine that we do not possess those labels by which we can learn the\n",
    "classification model. Still, we could find\n",
    "some pattern within the data itself. That is, let the data describe itself. This is what\n",
    "we will do in this chapter, where we consider the challenge of a question and answer\n",
    "website. When a user is browsing our site, perhaps because they were searching for\n",
    "particular information, the search engine will most likely point them to a specific\n",
    "answer. If the presented answers are not what they were looking for, the website\n",
    "should present (at least) the related answers so that they can quickly see what other\n",
    "answers are available and hopefully stay on our site."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-reader",
   "metadata": {},
   "source": [
    "We will achieve this goal in this chapter using clustering. This is a method of\n",
    "arranging items so that similar items are in one cluster and dissimilar items are in\n",
    "distinct ones. The tricky thing that we have to tackle first is how to turn text into\n",
    "something on which we can calculate similarity. With such a similarity measurement,\n",
    "we will then proceed to investigate how we can leverage that to quickly arrive at a\n",
    "cluster that contains similar posts. Once there, we will only have to check out those\n",
    "documents that also belong to that cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-newcastle",
   "metadata": {},
   "source": [
    "<h3>Measuring the relatedness of posts</h3><br/>\n",
    "From the machine learning point of view, raw text is useless. Only if we manage to transform it inot meaningul numbers, can we then feed it into our machine learning algorithms, suchs as clustering. This is true for more mundance operations on text such as similarity measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-territory",
   "metadata": {},
   "source": [
    "<h1> How to do it </h1><br/>\n",
    "More robust than edit distance is the so-called bag of word approach. It totally\n",
    "ignores the order of words and simply uses word counts as their basis. For each\n",
    "word in the post, its occurrence is counted and noted in a vector. \n",
    "Take, for instance, two example posts with the following word counts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-sheet",
   "metadata": {},
   "source": [
    "![alt text](bagofword.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-content",
   "metadata": {},
   "source": [
    "The columns Occurrences in post 1 and Occurrences in post 2 can now be treated as\n",
    "simple vectors. We can simply calculate the Euclidean distance between the vectors\n",
    "of all posts and take the nearest one (too slow, as we have found out earlier). And as\n",
    "such, we can use them later as our feature vectors in the clustering steps according to\n",
    "the following procedure:\n",
    "1. Extract salient features from each post and store it as a vector per post.\n",
    "2. Then compute clustering on the vectors.\n",
    "3. Determine the cluster for the post in question.\n",
    "4. From this cluster, fetch a handful of posts having a different similarity to the\n",
    "post in question. This will increase diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-wesley",
   "metadata": {},
   "source": [
    "<h1>Preprocessing - similarity measure as a similar number of common words</h1></br>\n",
    "The bag of word approach is both fast and robust. It is, though, not without challenges. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-canberra",
   "metadata": {},
   "source": [
    "<h3>Converting raw text into a bag of words</h3></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-albany",
   "metadata": {},
   "source": [
    "<b>Scikit's</b> <u>CountVectorizer</u> method counts the words and represents those counts as a vector not only efficiently but also with a very convenient interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "varied-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "following-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-lounge",
   "metadata": {},
   "source": [
    "The <b>min_df</b> parameter determines how <b>CountVectorizer</b> treats seldom words (minimum document frequency). If it is set to an integer, all words occurring less than that value will be dropped. If it is a fraction, all words that occur in less than that fraction of the overall dataset will be dropped. The <b>max_df</b> parameter works in a similar manner. If we print the instance, we see what other parameters SciKit provides together with their default values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "informational-webmaster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "checked-asthma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(?u)\\\\b\\\\w\\\\w+\\\\b'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.token_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-nightmare",
   "metadata": {},
   "source": [
    "Here, we can see that, the counting is done at word level  and that the words are determined by the regular expression pattern. It will, for example, tokenize \"cross-validated\" into \"cross\" and \"validated\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "indian-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = ['How to format my hard disk', 'Hard disk format problems ']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-baseline",
   "metadata": {},
   "source": [
    "We can now put this list of subject lines into the fit_transform() function of our vectorizer, which does all the hard vectorization work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "other-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "piano-telling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['disk', 'format', 'hard', 'how', 'my', 'problems', 'to']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aboriginal-relative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-strike",
   "metadata": {},
   "source": [
    "This means that the first sentence contains all the words except \"problems\", while the second contains all but \"how\", \"my\", and \"to\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-administration",
   "metadata": {},
   "source": [
    "<h3>Counting Words</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "chicken-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "paths = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "integrated-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = os.path.join(paths, 'toy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dependent-shark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\KU\\\\7th Sem\\\\Machine Learning\\\\ML_Practicals\\\\toy'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "democratic-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = [open(os.path.join(DIR, f)).read() for f in os.listdir(DIR)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "referenced-commission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a toy post about machine learning. Actually, it contains not much interesting stuff.',\n",
       " 'Imaging databases provide storage capabilities.',\n",
       " 'Most imaging databases save images permanently.\\n',\n",
       " 'Imaging databases store data.',\n",
       " 'Imaging databases store data. Imaging databases store data. Imaging databases store data.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "polished-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-reasoning",
   "metadata": {},
   "source": [
    "We have to notify the vectorizer about the full dataset so that it knows upfront what words are to be expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "occupied-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "brave-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples, num_features = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "described-paraguay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Samples: 5 & No. of Features: 25\n"
     ]
    }
   ],
   "source": [
    "print(f\"No. of Samples: {num_samples} & No. of Features: {num_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "linear-chaos",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'provide', 'save', 'storage', 'store', 'stuff', 'this', 'toy']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-nigeria",
   "metadata": {},
   "source": [
    "Now, we can vectorize our new post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "wicked-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = \"imaging databases\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "monetary-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-capitol",
   "metadata": {},
   "source": [
    "Note that the count vectors returned by the transform method are sparse. That is,\n",
    "each vector does not store one count value for each word, as most of those counts\n",
    "will be zero (the post does not contain the word). Instead, it uses the more memoryefficient implementation coo_matrix (for \"COOrdinate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "falling-gallery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n"
     ]
    }
   ],
   "source": [
    "print(new_post_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-mixture",
   "metadata": {},
   "source": [
    "Via its <b>toarray()</b> method, we can once again access the full ndarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "united-quest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(new_post_vec.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-territory",
   "metadata": {},
   "source": [
    "We need to use the full array, if we want to use it as a vector for similarity calculations. For the similarity measurement (the naive one), we calculate the Euclidean distance between the count vectors of the new post and all the old posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "labeled-advocacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "infectious-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_raw(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-software",
   "metadata": {},
   "source": [
    "The <b>norm()</b> function calculates the Euclidean norm(shortest distance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-pillow",
   "metadata": {},
   "source": [
    "With <b>dist_raw</b> we just need to iterate over all the posts and remember the nearest one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "declared-breed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=4.00:This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.73:Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=2.00:Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist=1.41:Imaging databases store data.\n",
      "=== Post 4 with dist=5.10:Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist = 1.41\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "for i in range(num_samples):\n",
    "    post=posts[i]\n",
    "    if post == new_post:\n",
    "        continue\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist_raw(post_vec, new_post_vec)\n",
    "    print(f\"=== Post {i} with dist={d:.2f}:{post}\")\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(f\"Best post is {best_i} with dist = {best_dist:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "educated-inclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.getrow(3).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dynamic-fabric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.getrow(4).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-binary",
   "metadata": {},
   "source": [
    "<h3>Normalizing Word Count Vectors</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-block",
   "metadata": {},
   "source": [
    "We will have to extend <b>dist_raw</b> to calculate the vector distance not on the raw vectors but on the normalized instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "rural-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "historic-shopping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.92: Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=0.77\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "for i in range(num_samples):\n",
    "    post=posts[i]\n",
    "    if post == new_post:\n",
    "        continue\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    print(\"=== Post %i with dist=%.2f: %s\"%(i, d, post))\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-september",
   "metadata": {},
   "source": [
    "Here, Post 3 and Post 4 are calculated as being equally similar. One could argue whether that much repitition would be a deligh to the reader, but from the point of counting the words in the posts this seems to be right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "faced-confidence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a toy post about machine learning. Actually, it contains not much interesting stuff.',\n",
       " 'Imaging databases provide storage capabilities.',\n",
       " 'Most imaging databases save images permanently.\\n',\n",
       " 'Imaging databases store data.',\n",
       " 'Imaging databases store data. Imaging databases store data. Imaging databases store data.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-swing",
   "metadata": {},
   "source": [
    "<h3>Removing less Important Words</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-carolina",
   "metadata": {},
   "source": [
    "Like in post 2, words such as \"most\" appear very often in all sorts of different contexts and are called stop words. They do not carry as much information and thus should not be weighed as much as words such as \"images\", which doesn't occur often in different contexts. The best option would be to remove all the words that are so frequent that they do not help to distinguish between difeerent texts. These words are called <b>Stop Words</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-degree",
   "metadata": {},
   "source": [
    "As this is such a common step in text processing, there is a simple parameter in CountVectorizer to achieve that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "danish-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-coating",
   "metadata": {},
   "source": [
    "If we have a list of words that we want to use as stop words, we can pass them as a list. But setting <b>stop_words</b> to english will use a set of 318 English stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "progressive-patent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(vectorizer.get_stop_words())[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "documentary-enterprise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 18\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples,num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "entire-harvard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'learning', 'machine', 'permanently', 'post', 'provide', 'save', 'storage', 'store', 'stuff', 'toy']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "unlimited-approach",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cooked-peter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n"
     ]
    }
   ],
   "source": [
    "print(new_post_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "through-station",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(new_post_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "described-seller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist = 1.41 : This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist = 0.86 : Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist = 0.86 : Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist = 0.77 : Imaging databases store data.\n",
      "=== Post 4 with dist = 0.77 : Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist = 0.77\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "for i in range(num_samples):\n",
    "    post=posts[i]\n",
    "    if post == new_post:\n",
    "        continue\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    print(f\"=== Post {i} with dist = {d:.2f} : {post}\")\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(f\"Best post is {best_i} with dist = {best_dist:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-usage",
   "metadata": {},
   "source": [
    "Wihtout stop words, Post 2 is now on par with Post 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-happening",
   "metadata": {},
   "source": [
    "<h3>Stemming</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-appraisal",
   "metadata": {},
   "source": [
    "We count similar words in different variants as different words. Post 2, for instance, contains \"imaging\" and \"images\". It will make sense to count them together. After all, it is the same concept they are referring to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-jungle",
   "metadata": {},
   "source": [
    "We need a function that reduces words to their specific word stem.Natural Language Tooolkit(NLTK), we can download a free software toolkit, which provides a stemmer that we can easily plug into CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "automated-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-investment",
   "metadata": {},
   "source": [
    "NLTK comes with different stemmers because every langugae has a different set of rules for stemming. For English, we can take SnowballStemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "constant-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "surprised-taiwan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graphic'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem('graphics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "grave-premium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imag'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem('imaging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "chicken-depth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imagin'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem('imagination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "flying-wiring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imagin'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem('imagine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dietary-style",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buy'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem('buys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "liked-asthma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buy'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem('buying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "national-buffalo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bought'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem('bought')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-student",
   "metadata": {},
   "source": [
    "<h3>Extending the vectorizer with NLTK's stemmer</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-missile",
   "metadata": {},
   "source": [
    "We need to stem the posts before we feed them into CountVectorizer. The class provides several hooks with which we can customize the stage's preprocessing and tokenization. The preprocessor and tokenizer can be set as parameters in the constructor. We do not want to place the stemmer into any of them, because we  will then have to do the tokenization and normalization by ourselves. Instead, we overwrite the <b>build_analyzer</b> method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "representative-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "foster-hunger",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "charming-mainland",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "vectorizer = StemmedCountVectorizer(min_df=1,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "expanded-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-notebook",
   "metadata": {},
   "source": [
    "This will do the following process for each post:\n",
    "1. The first step is lower casing the raw post in the preprocessing step\n",
    "(done in the parent class).\n",
    "2. Extracting all individual words in the tokenization step (done in the\n",
    "parent class).\n",
    "3. This concludes with converting each word into its stemmed version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cooked-lottery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 17\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples,num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "anonymous-malawi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actual', 'capabl', 'contain', 'data', 'databas', 'imag', 'interest', 'learn', 'machin', 'perman', 'post', 'provid', 'save', 'storag', 'store', 'stuff', 'toy']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "thorough-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dated-walnut",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n"
     ]
    }
   ],
   "source": [
    "print(new_post_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "crucial-dining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(new_post_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "pleasant-button",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist = 1.41 : This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist = 0.86 : Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist = 0.63 : Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist = 0.77 : Imaging databases store data.\n",
      "=== Post 4 with dist = 0.77 : Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 2 with dist = 0.63\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "for i in range(num_samples):\n",
    "    post=posts[i]\n",
    "    if post == new_post:\n",
    "        continue\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    print(f\"=== Post {i} with dist = {d:.2f} : {post}\")\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(f\"Best post is {best_i} with dist = {best_dist:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-heading",
   "metadata": {},
   "source": [
    "<h3>Stop words on steroids</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-cotton",
   "metadata": {},
   "source": [
    "We want a high value for a given term in a given value, if that term occurs often in that particular post and very seldom anywhere else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-federal",
   "metadata": {},
   "source": [
    "This is exactly what term frequency - inverse document frequency (TF-IDF) des. TF stands for the counting part, while IDF factors in the discounting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "split-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "amateur-hundred",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(term, doc, corpus):\n",
    "    tf = doc.count(term)/ len(doc)\n",
    "    num_docs_with_term = len([d for d in corpus if term in d])\n",
    "    idf = sp.log(len(corpus)/num_docs_with_term)\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-cache",
   "metadata": {},
   "source": [
    "Here, we simply did not count the terms only, but also normalize the counts by the document length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "verbal-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, abb, abc = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "historic-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = [a, abb, abc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "british-importance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-59-123a228042b0>:4: DeprecationWarning: scipy.log is deprecated and will be removed in SciPy 2.0.0, use numpy.lib.scimath.log instead\n",
      "  idf = sp.log(len(corpus)/num_docs_with_term)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"a\", a, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "unauthorized-bowling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-59-123a228042b0>:4: DeprecationWarning: scipy.log is deprecated and will be removed in SciPy 2.0.0, use numpy.lib.scimath.log instead\n",
      "  idf = sp.log(len(corpus)/num_docs_with_term)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"a\", abb, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "liable-thing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-59-123a228042b0>:4: DeprecationWarning: scipy.log is deprecated and will be removed in SciPy 2.0.0, use numpy.lib.scimath.log instead\n",
      "  idf = sp.log(len(corpus)/num_docs_with_term)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"a\", abc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "virgin-hartford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27031007207210955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-59-123a228042b0>:4: DeprecationWarning: scipy.log is deprecated and will be removed in SciPy 2.0.0, use numpy.lib.scimath.log instead\n",
      "  idf = sp.log(len(corpus)/num_docs_with_term)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"b\", abb, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "valued-watson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-59-123a228042b0>:4: DeprecationWarning: scipy.log is deprecated and will be removed in SciPy 2.0.0, use numpy.lib.scimath.log instead\n",
      "  idf = sp.log(len(corpus)/num_docs_with_term)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"a\", abc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "hydraulic-location",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13515503603605478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-59-123a228042b0>:4: DeprecationWarning: scipy.log is deprecated and will be removed in SciPy 2.0.0, use numpy.lib.scimath.log instead\n",
      "  idf = sp.log(len(corpus)/num_docs_with_term)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"b\", abc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "located-grenada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3662040962227032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-59-123a228042b0>:4: DeprecationWarning: scipy.log is deprecated and will be removed in SciPy 2.0.0, use numpy.lib.scimath.log instead\n",
      "  idf = sp.log(len(corpus)/num_docs_with_term)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"c\", abc, D))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-subscriber",
   "metadata": {},
   "source": [
    "We see that a carries no meaning for any document since it is contained everywhere. The b term is more important for the document abb than for abc as it occurs there twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "silent-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "vital-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc : (\n",
    "            english_stemmer.stem(w) for w in analyzer(doc)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "marked-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(min_df=1, \n",
    "                                    stop_words='english',\n",
    "                                    decode_error='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "varied-equity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 17\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "print(f\"#samples: {num_samples}, #features: {num_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "piano-slovak",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t0.7071067811865476\n",
      "  (0, 4)\t0.7071067811865476\n"
     ]
    }
   ],
   "source": [
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "print(new_post_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "unable-geneva",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist = 1.41 : This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist = 1.08 : Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist = 0.86 : Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist = 0.92 : Imaging databases store data.\n",
      "=== Post 4 with dist = 0.92 : Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 2 with dist = 0.86\n"
     ]
    }
   ],
   "source": [
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "\n",
    "for i, post in enumerate(posts):\n",
    "    if post == new_post:\n",
    "        continue\n",
    "        \n",
    "    post_vec = X_train.getrow(i)\n",
    "    \n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    \n",
    "    print(f\"=== Post {i} with dist = {d:.2f} : {post}\")\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i \n",
    "        \n",
    "print(f\"Best post is {best_i} with dist = {best_dist:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-avatar",
   "metadata": {},
   "source": [
    "<h3>Clustering</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-hollywood",
   "metadata": {},
   "source": [
    "Finally, we have our vectors, which we believe capture the posts to a sufficient degree.\n",
    "Not surprisingly, there are many ways to group them together. Most clustering\n",
    "algorithms fall into one of the two methods: flat and hierarchical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-riverside",
   "metadata": {},
   "source": [
    "<b>Flat Clustering: </b>Flat clustering divides the posts into a set of clusters without relating the clusters to\n",
    "each other. The goal is simply to come up with a partitioning such that all posts in\n",
    "one cluster are most similar to each other while being dissimilar from the posts in all\n",
    "other clusters. Many flat clustering algorithms require the number of clusters to be\n",
    "specified up front."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-stupid",
   "metadata": {},
   "source": [
    "<b>Hierarchial Clustering: </b>In hierarchical clustering, the number of clusters does not have to be specified.\n",
    "Instead, hierarchical clustering creates a hierarchy of clusters. While similar posts\n",
    "are grouped into one cluster, similar clusters are again grouped into one uber-cluster.\n",
    "This is done recursively, until only one cluster is left that contains everything. In\n",
    "this hierarchy, one can then choose the desired number of clusters after the fact.\n",
    "However, this comes at the cost of lower efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-trout",
   "metadata": {},
   "source": [
    "<h3>K-means Clustering</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-medication",
   "metadata": {},
   "source": [
    "k-means is the most widely used flat clustering algorithm. After initializing it with\n",
    "the desired number of clusters, num_clusters, it maintains that number of so-called\n",
    "cluster centroids. Initially, it will pick any num_clusters posts and set the centroids\n",
    "to their feature vector. Then it will go through all other posts and assign them the\n",
    "nearest centroid as their current cluster. Following this, it will move each centroid\n",
    "into the middle of all the vectors of that particular class. This changes, of course, the\n",
    "cluster assignment. Some posts are now nearer to another cluster. So it will update\n",
    "the assignments for those changed posts. This is done as long as the centroids move\n",
    "considerably. After some iterations, the movements will fall below a threshold and\n",
    "we consider clustering to be converged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-construction",
   "metadata": {},
   "source": [
    "<h3>Getting test data to evaluate our ideas on </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "modern-vienna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "all_data = sklearn.datasets.fetch_20newsgroups(subset='all')\n",
    "len(all_data.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "mexican-annex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(all_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "sought-engineering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n"
     ]
    }
   ],
   "source": [
    "train_data = sklearn.datasets.fetch_20newsgroups(subset='train')\n",
    "print(f\"{len(train_data.filenames)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "indirect-mineral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7532\n"
     ]
    }
   ],
   "source": [
    "test_data = sklearn.datasets.fetch_20newsgroups(subset='test')\n",
    "print(f\"{len(test_data.filenames)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "sunset-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['comp.graphics', 'comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware','comp.windows.x', 'sci.space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "descending-momentum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3529\n"
     ]
    }
   ],
   "source": [
    "train_data = sklearn.datasets.fetch_20newsgroups(subset='train', categories=groups)\n",
    "print(len(train_data.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "auburn-patio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2349\n"
     ]
    }
   ],
   "source": [
    "test_data = sklearn.datasets.fetch_20newsgroups(subset='test', categories=groups)\n",
    "print(len(test_data.filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-theology",
   "metadata": {},
   "source": [
    "<h2>Clustering Posts</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-clone",
   "metadata": {},
   "source": [
    "The newsgropu dataset contains invalid characters that will result in UnicodeDecorder so we have to tell the vectorizer to ignore them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "related-naples",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(min_df=10, max_df=0.5,\n",
    "                                   stop_words='english',\n",
    "                                   decode_error='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "inner-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = vectorizer.fit_transform(train_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "blond-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples, num_features = vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "linear-boxing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of samples: 3529 & No. of features: 4712\n"
     ]
    }
   ],
   "source": [
    "print(f\"No. of samples: {num_samples} & No. of features: {num_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-clinic",
   "metadata": {},
   "source": [
    "So we have a pool of 3529 posts and have extracted for each of them a feature vetor of 4712 dimensions. That is what K-means takes an input. We will fix the cluster size to 50 for this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "latest-examination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration 0, inertia 5899.5595831471655\n",
      "Iteration 1, inertia 3218.297747726279\n",
      "Iteration 2, inertia 3184.3328334733214\n",
      "Iteration 3, inertia 3164.867358130041\n",
      "Iteration 4, inertia 3152.003949571175\n",
      "Iteration 5, inertia 3143.1109963529184\n",
      "Iteration 6, inertia 3136.2559774422048\n",
      "Iteration 7, inertia 3129.3248717684405\n",
      "Iteration 8, inertia 3124.5674798201394\n",
      "Iteration 9, inertia 3121.9001105797406\n",
      "Iteration 10, inertia 3120.209894571872\n",
      "Iteration 11, inertia 3118.62745619288\n",
      "Iteration 12, inertia 3117.3625259783616\n",
      "Iteration 13, inertia 3116.8112664390364\n",
      "Iteration 14, inertia 3116.587892365764\n",
      "Iteration 15, inertia 3116.417048753848\n",
      "Iteration 16, inertia 3115.760414808626\n",
      "Iteration 17, inertia 3115.3736535034473\n",
      "Iteration 18, inertia 3115.155454436256\n",
      "Iteration 19, inertia 3114.949117560754\n",
      "Iteration 20, inertia 3114.5149932662175\n",
      "Iteration 21, inertia 3113.9369169464094\n",
      "Iteration 22, inertia 3113.719999300366\n",
      "Iteration 23, inertia 3113.547519005385\n",
      "Iteration 24, inertia 3113.474905847476\n",
      "Iteration 25, inertia 3113.4467573371267\n",
      "Converged at iteration 25: strict convergence.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(init='random', n_clusters=50, n_init=1, random_state=3, verbose=1)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusters = 50\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=num_clusters, init='random', n_init=1,\n",
    "           verbose=1, random_state=3)\n",
    "km.fit(vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-polymer",
   "metadata": {},
   "source": [
    "We provided a random state just so that we can get the same results. In real-world applications, we will not do this. After fitting, we can get the clustering information out of memebrs of km. For every vectorized post that has been fit, there is a corresponding integer label in km.labels_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "martial-thomas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38 17 47 ... 41 14 16]\n"
     ]
    }
   ],
   "source": [
    "print(km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "independent-exception",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3529,)\n"
     ]
    }
   ],
   "source": [
    "print(km.labels_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-surge",
   "metadata": {},
   "source": [
    "The cluster centers can be accessed via <b>km.cluster_centers_</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-candy",
   "metadata": {},
   "source": [
    "<h3>Solving our initial challenge</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "attended-kansas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Disk drive problems. Hi, I have a problem with my hard disk. After 1 year it is working only sporadically now.I tried to format it, but now it doesn't boot any more.Any ideas? Thanks.\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post = \"Disk drive problems. Hi, I have a problem with my hard \\\n",
    "disk. After 1 year it is working only sporadically now.\\\n",
    "I tried to format it, but now it doesn't boot any more.\\\n",
    "Any ideas? Thanks.\"\n",
    "new_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "aggressive-factory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post_vec = vectorizer.transform([new_post])\n",
    "new_post_label = km.predict(new_post_vec)[0]\n",
    "new_post_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "healthy-weight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-metallic",
   "metadata": {},
   "source": [
    "Now that we have the clustering, we do not need to compare new_post_vec to all post vectors. Instead, we can focus only on the posts of the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "handmade-electronics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_indices = (km.labels_==new_post_label).nonzero()[0]\n",
    "len(similar_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-consolidation",
   "metadata": {},
   "source": [
    "The comparison in the bracket results in a Boolean array, and nonzero converts that\n",
    "array into a smaller array containing the indices of the True elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-convertible",
   "metadata": {},
   "source": [
    "Using similar_indices, we then simply have to build a list of posts together with\n",
    "their similarity scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "north-domain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "similar = []\n",
    "\n",
    "for i in similar_indices:\n",
    "    dist = np.linalg.norm((new_post_vec - vectorized[i]).toarray())\n",
    "    similar.append((dist, train_data.data[i]))\n",
    "                   \n",
    "similar = sorted(similar)\n",
    "len(similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "going-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_at_1 = similar[0]\n",
    "show_at_2 = similar[int(len(similar)/10)]\n",
    "show_at_3 = similar[int(len(similar)/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "copyrighted-service",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0378441731334072 \t From: Thomas Dachsel <GERTHD@mvs.sas.com>\n",
      "Subject: BOOT PROBLEM with IDE controller\n",
      "Nntp-Posting-Host: sdcmvs.mvs.sas.com\n",
      "Organization: SAS Institute Inc.\n",
      "Lines: 25\n",
      "\n",
      "Hi,\n",
      "I've got a Multi I/O card (IDE controller + serial/parallel\n",
      "interface) and two floppy drives (5 1/4, 3 1/2) and a\n",
      "Quantum ProDrive 80AT connected to it.\n",
      "I was able to format the hard disk, but I could not boot from\n",
      "it. I can boot from drive A: (which disk drive does not matter)\n",
      "but if I remove the disk from drive A and press the reset switch,\n",
      "the LED of drive A: continues to glow, and the hard disk is\n",
      "not accessed at all.\n",
      "I guess this must be a problem of either the Multi I/o card\n",
      "or floppy disk drive settings (jumper configuration?)\n",
      "Does someone have any hint what could be the reason for it.\n",
      "Please reply by email to GERTHD@MVS.SAS.COM\n",
      "Thanks,\n",
      "Thomas\n",
      "+-------------------------------------------------------------------+\n",
      "| Thomas Dachsel                                                    |\n",
      "| Internet: GERTHD@MVS.SAS.COM                                      |\n",
      "| Fidonet:  Thomas_Dachsel@camel.fido.de (2:247/40)                 |\n",
      "| Subnet:   dachsel@rnivh.rni.sub.org (UUCP in Germany, now active) |\n",
      "| Phone:    +49 6221 4150 (work), +49 6203 12274 (home)             |\n",
      "| Fax:      +49 6221 415101                                         |\n",
      "| Snail:    SAS Institute GmbH, P.O.Box 105307, D-W-6900 Heidelberg |\n",
      "| Tagline:  One bad sector can ruin a whole day...                  |\n",
      "+-------------------------------------------------------------------+\n",
      "\n",
      "-------------------\n",
      "1.1716497460538475 \t From: badry@cs.UAlberta.CA (Badry Jason Theodore)\n",
      "Subject: Chaining IDE drives\n",
      "Summary: Trouble with Master/Slave drives\n",
      "Nntp-Posting-Host: cab009.cs.ualberta.ca\n",
      "Organization: University Of Alberta, Edmonton Canada\n",
      "Lines: 16\n",
      "\n",
      "Hi.  I am trying to set up a Conner 3184 and a Quantum 80AT drive.  I have\n",
      "the conner set to the master, and the quantum set to the slave (doesn't work\n",
      "the other way around).  I am able to access both drives if I boot from a \n",
      "floppy, but the drives will not boot themselves.  I am running MSDOS 6, and\n",
      "have the Conner partitioned as Primary Dos, and is formatted with system\n",
      "files.  I have tried all different types of setups, and even changed IDE\n",
      "controller cards.  If I boot from a floppy, everything works great (except\n",
      "the booting part :)).  The system doesn't report an error message or anything,\n",
      "just hangs there.  Does anyone have any suggestions, or has somebody else\n",
      "run into a similar problem?  I was thinking that I might have to update the bios\n",
      "on one of the drives (is this possible?).  Any suggestions/answers would be\n",
      "greatly appreciated.  Please reply to:\n",
      "\n",
      "\tJason Badry\n",
      "\tbadry@cs.ualberta.ca\n",
      "\n",
      "\n",
      "-------------------\n",
      "1.1716497460538475 \t From: badry@cs.UAlberta.CA (Badry Jason Theodore)\n",
      "Subject: Chaining IDE drives\n",
      "Summary: Trouble with Master/Slave drives\n",
      "Nntp-Posting-Host: cab009.cs.ualberta.ca\n",
      "Organization: University Of Alberta, Edmonton Canada\n",
      "Lines: 16\n",
      "\n",
      "Hi.  I am trying to set up a Conner 3184 and a Quantum 80AT drive.  I have\n",
      "the conner set to the master, and the quantum set to the slave (doesn't work\n",
      "the other way around).  I am able to access both drives if I boot from a \n",
      "floppy, but the drives will not boot themselves.  I am running MSDOS 6, and\n",
      "have the Conner partitioned as Primary Dos, and is formatted with system\n",
      "files.  I have tried all different types of setups, and even changed IDE\n",
      "controller cards.  If I boot from a floppy, everything works great (except\n",
      "the booting part :)).  The system doesn't report an error message or anything,\n",
      "just hangs there.  Does anyone have any suggestions, or has somebody else\n",
      "run into a similar problem?  I was thinking that I might have to update the bios\n",
      "on one of the drives (is this possible?).  Any suggestions/answers would be\n",
      "greatly appreciated.  Please reply to:\n",
      "\n",
      "\tJason Badry\n",
      "\tbadry@cs.ualberta.ca\n",
      "\n",
      "\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "for i in [show_at_1, show_at_2, show_at_2]:\n",
    "    print(f\"{i[0]} \\t {i[1]}\")\n",
    "    print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-bloom",
   "metadata": {},
   "source": [
    "<h3>Another look at noise</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "changed-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_group = zip(train_data.data, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "naughty-traffic",
   "metadata": {},
   "outputs": [],
   "source": [
    "all = [(len(post[0]), post[0], train_data.target_names[post[1]]) for post in post_group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "handy-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphics = sorted([post for post in all if post[2] == 'comp.graphics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "variable-tissue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(245, 'From: SITUNAYA@IBM3090.BHAM.AC.UK\\nSubject: test....(sorry)\\nOrganization: The University of Birmingham, United Kingdom\\nLines: 1\\nNNTP-Posting-Host: ibm3090.bham.ac.uk\\n\\n==============================================================================\\n', 'comp.graphics')\n"
     ]
    }
   ],
   "source": [
    "print(graphics[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ordered-buffer",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_post = graphics[5][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "economic-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = vectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "annoying-handling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['situnaya', 'ibm3090', 'bham', 'ac', 'uk', 'subject', 'test', 'sorri', 'organ', 'univers', 'birmingham', 'unit', 'kingdom', 'line', 'nntp', 'post', 'host', 'ibm3090', 'bham', 'ac', 'uk']\n"
     ]
    }
   ],
   "source": [
    "print(list(analyzer(noise_post)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "honest-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "useful = set(analyzer(noise_post)).intersection(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "impossible-exhibition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ac', 'birmingham', 'host', 'kingdom', 'nntp', 'sorri', 'test', 'uk', 'unit', 'univers']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(useful))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "packed-singing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF(ac) = 3.51    \n",
      "IDF(birmingham) = 6.77    \n",
      "IDF(host) = 1.74    \n",
      "IDF(kingdom) = 6.68    \n",
      "IDF(nntp) = 1.77    \n",
      "IDF(sorri) = 4.14    \n",
      "IDF(test) = 3.83    \n",
      "IDF(uk) = 3.70    \n",
      "IDF(unit) = 4.42    \n",
      "IDF(univers) = 1.91    \n"
     ]
    }
   ],
   "source": [
    "for term in sorted(useful):\n",
    "    print(f\"IDF({term}) =\\\n",
    " {vectorizer._tfidf.idf_[vectorizer.vocabulary_[term]]:.2f}\\\n",
    "    \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
